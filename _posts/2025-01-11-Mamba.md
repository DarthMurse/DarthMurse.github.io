---
title: 'Mamba'
date: 2025-1-11
permalink: /posts/2025/01/11/blog-post-1/
tags:
  - Linear Attention
---

Mamba是一个于2023年由卡耐基梅隆大学的Albert Gu和Tri Dao提出的选择状态空间模型（Selective State Space Model），可以用于语言建模、序列建模、图像识别等诸多下游任务，是一个以取代Transformer为目标的基础模型结构。和RWKV一样，Mamba对序列长度具有线性计算复杂度，同时拥有并行训练、串行推理的优势，引起了很高的关注度。

## Mamba
既然Mamba是一个状态空间模型，那这个名字究竟从何而来呢？Mamba的作者在[论文](https://arxiv.org/abs/2312.00752)中解释说这是因为他们设计的模型快速又高效，就像黑曼巴蛇一样，因此取名为Mamba。（但我只是怀疑他们在玩梗...想你了，牢大）从名字中我们可以知道，Mamba是一种带有选择性的状态空间模型，那么什么是状态空间模型呢？简单地说，状态空间模型在进行预测时不仅以来当前时刻的输入，同时还依赖于模型内部保存的一个隐藏状态量，其中包含了关于过往输入的信息。是不是有点像RNN？没错，其实Mamba也是一种特殊的RNN。在描述自然界的许多物理过程时，我们也常常会用到状态空间模型，只不过一般是通过微分方程的形式

$$\frac{dh(t)}{dt} = Ah(t) + Bx(t)$$

$$y(t) = Ch(t) + Dx(t)$$

当然，在神经网络中我们不能使用连续时间的微分方程，因此需要对上述的常微分方程进行离散化。这个离散化的方法有很多种，Mamba中采用的是零阶保持器方法。什么是零阶保持器呢？顾名思义，就是假设输入信号$$x(t)$$在短时间之内保持不变。由于状态方程组是一个一阶非其次线性方程组，因此其通解可表示为

$$h(t) = Ce^{At} + e^{At}\int_0^te^{-\tau}Bx(\tau)d\tau.$$

为了简单起见，我们设$$C=1$$，那么离散方程中前一时刻$$h_{t-1} = h(t - \Delta t)$$，带入上面的式子可以得到

$$h(t) - e^{\Delta A}h_{t-1} = e^{At}\int_{t-\Delta t}^te^{-A\tau}Bx(\tau)d\tau.$$

这时候我们的零阶保持假设，即$$x(\tau) = x(t), \tau \in [t-\Delta t, t]$$就发挥了作用，让我们可以把积分给去掉，化简之后得到

$$h_t = \bar{A}h_{t-1} + \bar{B}x_t,$$

$$\bar{A} = e^{\Delta A}, \bar{B} = (\Delta A)^{-1}(e^{\Delta A}-I)\cdot \Delta B.$$

这就是我们离散化之后的递推式，也是Mamba中核心模块的计算过程。其实通过上面的推导我们发现如果只关心神经网络中关于$$h_t, h_{t-1}, x_t$$的递推式，根本就不需要从$$A, B$$出发再计算得到$$\bar{A}, \bar{B}$$，而只需要将$$\bar{A}, \bar{B}$$作为学习参数即可。但是这样与连续的微分方程系数相联系的做法可能能够使网络更容易获得一个比较好的初始化参数，或者是能作为一种规范化（normalization）的方法，使得网络的训练更加稳定。

现在我们获得了递推式，但这是一个串行计算的公式，如果我们的序列长度很长，比如大语言模型中动辄1000以上的上下文长度（context length），那么我们的训练速度将会非常缓慢。为此，Mamba的作者采用了一种并行扫描（Parallel Scan）的算法，将串行的计算速度从$$O(n)$$缩减到了$$O(log \ n)$$，其中$$n$$为序列的长度。这个算法的具体过程在这里我就不多赘述了，如果读者感兴趣的话可以参考一下[mamba.py](https://github.com/alxndrTL/mamba.py/blob/main/mambapy/mamba.py)中的代码和[Prefix Sums and Their Applications](https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf)这本书中的内容。

此外Mamba的作者还设计了一套优化显存加速计算的kernel算子（他们团队做这个真的很厉害，Flash-Attention也是他们的工作），不过我对此了解不深，水平有限就不过多阐述了，有兴趣的朋友自己去看代码哦，这里贴个原仓库的[链接](https://github.com/state-spaces/mamba)

### 网络结构
介绍完了Mamba的核心部分，我们再来看一看Mamba整体的网络结构。和Transformer模型一样，Mamba模型的输入为维度为$$[B, L, D]$$的张量，其中$$B$$为batch\_size，$$L$$为序列长度，$$D$$为特征大小。Mamba网络的整体结构如下图所示

![Mamba结构](https://darthmurse.github.io/images/mamba-all.png)

其中Conv为对特征维度$$D$$进行的一维卷积（Conv1D），$$\sigma$$为silu激活函数，网络结构中加入了RMSNorm和残差连接确保训练的稳定性，防止梯度爆炸和梯度消失。其中核心的SSM模块大体的计算方式就是我们之前介绍的离散化状态空间模型，具体的计算过程如下图所示

![SSM模块结构](https://darthmurse.github.io/images/mamba-ssm.png)

其中$$N$$为隐藏状态空间的维度。在所有的网络参数中，需要特别注意$$\Delta$$和A的初始化方法。根据我们之前的推导，我们需要确保$$\Delta$$是一个较小的正数张量，因此我们需要对$$\Delta$$之前的线性层偏置（bias）做一些限制（具体参加mamba.py的代码）；同时注意到A和$$\Delta$$在逐元素相乘之后会经过一个指数操作，因此必须保证A的初始化值不超过一个范围，以防止指数爆炸。

### 语言任务效果
Mamba的作者在语言建模、DNA预测、语音识别等一系列任务上对Mamba的效果进行了测试。在语言建模任务上，Mamba在同参数量的情况下全面超过了使用Transformer结构的Pythia系列模型，也因此吸引了很大的关注。

![Mamba语言建模效果](https://blog.premai.io/content/images/size/w1000/2024/01/Untitled--15-.png)

与状态转移矩阵不随时间$$t$$改变的S4模型相比，Mamba选择状态模型S6在语言建模上取得了显著的进步

![S4和S6](https://darthmurse.github.io/images/mamba-ablation.png)

作者解释说这是因为S4模型的先验假设是一个连续的微分方程，因此不太擅长离散数据的语言建模任务，让转移状态矩阵中的$$B$$和$$C$$与输入相关从一定程度上弥补了S4的这个缺点，因此能够在语言任务上获得更好的结果，但相应地，其表现在连续数据的场景下就会有所下降，比如声波拟合的任务。

### 一些思考和总结
尽管现在涌现的诸多线性注意力类RNN模型能够在语言建模上取得比较不错的效果，甚至超过一些Transformer的模型（可怜的Pythia，天天被当靶子），但是有研究表明，在长序列的语言任务上（上下文长度8000以上），Transformer仍然具有更大的优势，因为它理论上能够拥有无限长的记忆能力，只是推理新token的速度也会因此而增长；而线性注意力模型虽然在推理新的token时速度不会随着上下文长度而变慢，但也因此必须对历史信息有所取舍。（其实我觉得人类的记忆可能反而更接近线性注意力的RNN——我们不也经常忘记东西吗。。。）

目前成功的线性注意力机制中基本都有类似Attention中Q，K，V的模块，用一个依赖于输入的算子去操作输入，而在传统的RNN，如GRU和LSTM中，这种类型的计算仅限于门控电路，表达能力非常有限，因此难以取得很好的效果。

最后，如此多类RNN网络的成功告诉我们，只要模型的设计符合一定的数据先验假设，同时通过设置合理的初始化参数和训练策略能够避免梯度爆炸和梯度消失，那么这个网络就大概率可以获得成功。多多尝试吧！