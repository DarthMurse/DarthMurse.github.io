---
title: 'My Opinions on Neuromorphic Computing'
date: 2025-6-6
permalink: /posts/2025/06/06/blog-post-1/
tags:
  - Neuromorphic Computing
---

类脑计算的尴尬处境

## 什么是类脑计算
类脑计算是一个非常模糊的概念，从名称上看，它代表了一大类有生物神经元启发的计算方法，因此，从严格意义上说，现在AI领域流行的各种全连接、卷积网络都可以算作是类脑计算。从历史上看，类脑计算这一概念最早起源于硬件领域，代表了一种从硬件上构建类似生物神经元的计算单元，从而实现类似大脑的功能的想法。然而，人类的大脑非常复杂，事实上，时至今日我们仍然对大脑的运算方式知之甚少，因此可以想象，使用电路构建一个类似大脑的计算系统在目前根本就是一个天方夜谭，因为我们连大脑是怎么工作的都不知道！既然无法直接构建一个“人工大脑”，类脑计算的目标就退而求其次，转而变成在通用计算领域在某些特定任务上胜过传统计算架构，就像GPU在并行计算任务上优于CPU那样。在传统的计算架构中（包括GPU和CPU），存储和计算单元在物理上是分开的，而计算指令中的一大部分资源都花在了数据的读取中，实际计算消耗的资源在一般情况下反而只占了一个零头（正因如此，大部分针对GPU的计算优化其实都在用各种方式降低读取的频率，增加数据复用）；为了解决这个问题，类脑计算引入了存算一体的概念，用一个电路单元同时完成计算和存储，正如生物神经元那样，而这也成为了类脑计算硬件的关键性特点。当然，这是一个全新的领域，所有的硬件设计、制造、测试，相应的指令集、软件工具链都需要从头开始设计。目前，存算一体的类脑硬件基本仍然还在硬件制造环节，但是非常具有潜力，一旦成功大概会带来数量级的速度和能效提升。由于存算一体芯片难于制造，目前还有另一种基于近存计算的类脑计算硬件架构，

## 关于类脑计算算法未来的发展
类脑计算的算法有两个主要特性，一是脉冲，二是动力学特性。我不想提生物可解释性，因为我是一个做计算的人，生物可解释性对我没有意义。

从现在已有的一些证据来看，脉冲除了能够在一定程度上提高系统的鲁棒性外几乎一无是处（当然你要和我谈功耗那当我没说），即便是在某些具有不连续特点的物理问题中，使用不连续的激活函数效果也会强于使用脉冲激活函数。多个时间步的脉冲直接编码（对脉冲序列取平均）效果也显著低于单时间步多脉冲发放的神经元（后者其实等同于量化ANN），这很好理解，在直接编码中，4个比特只能编码0-4的整数，而在后者中，则能编码0-15的整数，二者表达信息的能力几乎差了4倍，对更高的时间步更是如此。其实这里有个很有意思的问题，就是如何分配比特数能够使神经网络达到最好的效果。我们知道，一般而言神经网络的结构会有宽度和深度两个超参（Transformer之类更复杂的网络会有更多结构超参），但其实还有另外一个超参，就是权重和激活的比特数。如何在一次前向传播中消耗同样次数的位运算的情况下，设计出性能最佳的超参配置，是个很有趣的问题，不过也很困难。但是，其实这样的超参优化方案在实际情况中可能并不现实，因为已有的计算硬件（GPU，CPU）等往往只支持特定位数的定点或浮点数运算，比如int8和bfloat16，英伟达在最新的硬件中开始支持int4，但是对超低精度的量化网络支持仍然有限。从硬件通用性的角度考虑，支持超低精度的计算未必是一件划算的事，而且从目前的训练方法来看，训练时的网络仍然需要至少16位的浮点数来保存梯度和权重信息。随着量化方法的发展，我相信未来训练硬件和部署硬件分离化的趋势会越来越明显。这里有一些扯远了，回到脉冲的表征方式，如果没有合适的硬件实现方式，所有的功耗提升都不过是空谈，而现在各种各样脉冲网络的设计缺乏一种统一的形式，使得真正的通用脉冲计算硬件极难设计，最后，为了计算的通用型，我们最终会发现自己设计了一种在数据表征方式上和传统硬件完全一致的硬件，而脉冲反倒成了可有可无的特性。这正是目前Liohi2，SpiNNaker2等类脑硬件所展现出来的趋势。我并不反对这种趋势，在我看来，对于能够精确计算存储信息的数字电路（没有噪声），脉冲的表达反而是一种累赘，生物神经元中采用脉冲传递信号在一定程度上是为了减少细胞复杂环境中的噪声对计算结果的影响，增加神经系统的鲁棒性，对与数字电路来说，这完全没有必要，反而降低了数据传输的效率。或许会有人争辩，脉冲可以带来事件驱动的特性，所谓事件驱动，就是仅有脉冲产生时才会进行数据计算和传输，在静息情况下不消耗能量。先不考虑静息态不消耗能量这一理想情况能否在数字电路中实现，单从信息表证的效率而言，从信息论的角度看，每个时刻有50%的概率发放脉冲时效率是最高的，实际上，当我们测试整数量化的网络时，确实可以发现每个比特位是1的概率基本上保持在50%，如果某个时刻脉冲发放的概率小于50%，其实是说明这个算法还有压缩优化的空间。对于一个脉冲效率最高（50%发放概率）的网络来说，事件驱动带来的计算效率提升至多达到2倍，算上为了实现事件驱动的其他开销，实际上可能得不偿失。在我的观点中，所有以功耗为卖点的类脑算法，都必须拿出实际的证据，要么能够在现有的硬件上真正部署实测，要么至少能提出一种理论上的硬件实现能够实现大致正确的估算（最好在FPGA上实现一下），否则这类研究的意义相当存疑；另一个方向是用脉冲，或者说是不连续性（不连续函数可以看作阶跃函数和另一个连续函数的乘积），在某些任务上取得相较于连续网络的提升（比如SpikingNeRF的工作），但是这样的任务很难找。

类脑计算的另一个特点动力学就比较好理解了，神经元本身就是用一个微分动力系统建模，而类脑计算中的脉冲神经网络通常的做法其实就是把这个连续的动力系统离散化。其实不只是SNN，RNN也具备明显的动力学特性，现在爆火的Mamba也是如此。动力学特性是一个很有潜力的研究方向，毕竟时间是我们这个世界的一个重要维度。但是动力学特性并不是类脑计算所独有的，

每当人们提及类脑计算，首先想到的便是脉冲神经网络，可我却想说，如今的大多数脉冲神经网络的工作，性能不如ANN，能耗没有确切降低的证据，训练方法也是ANN中常见的梯度下降，一切都是小领域的自娱自乐。很多工作甚至只是ANN的量化工作换了个皮，既然如此，为什么不直接把自己归为量化方法呢？而本应更加重要的学习机制、记忆机制，却又被归入了ANN的范畴，实在是有舍本逐末之感。而且，说到底AI不就是从神经科学中启发而来吗？为什么一定要再圈出一个小圈子自称为类脑计算？这真的很令人费解。

我决定了，以后介绍研究方向，只说AI，不说类脑计算。